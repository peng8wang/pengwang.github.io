<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="lai.css" type="text/css" />
<title>Peng Wang</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Peng Wang</div>
<div class="menu-item"><a href="index.html" class="current"><b>Home</b></a></div>
<div class="menu-item"><a href="activity.html"><b>Activities</b></a></div>
<div class="menu-item"><a href="teaching.html"><b>Teaching</b></a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Peng Wang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="tianlai.jpg" alt="Peng Wang Photo" width="150px" />&nbsp;</td>
<td align="left"><p>Peng Wang <br />
Postdoc Research Fellow <br />
<a href="https://eecs.engin.umich.edu/" target=&ldquo;blank&rdquo;><b>Department of Electrical Engineering and Computer Science</b></a> <br /> 
<a href="https://umich.edu/" target=&ldquo;blank&rdquo;><b>University of Michigan, Ann Arbor</b></a> <br />
<b>Email</b>: pengwa@umich.edu <br />
<a href="https://scholar.google.com/citations?user=baF3HKUAAAAJ&amp;hl=zh-TW" target=&ldquo;blank&rdquo;><b>Google Scholar</b></a>, <a href="https://orcid.org/0000-0002-6799-0745" target=&ldquo;blank&rdquo;><b>ORCDI</b></a>
</p>
</td></tr></table>
<h2>About Me</h2>
<p>I am currently a postdoc research fellow advised by Professors <a href="https://web.eecs.umich.edu/~girasole/" target=&ldquo;blank&rdquo;><b>Laura Balzano</b></a> and <a href="https://qingqu.engin.umich.edu/" target=&ldquo;blank&rdquo;><b>Qing Qu</b></a> at <a href="https://umich.edu/" target=&ldquo;blank&rdquo;><b>University of Michigan</b></a>. Before that, I got my Ph.D. degree in Systems Engineering and Engineering Management advised by Professor <a href="https://www1.se.cuhk.edu.hk/~manchoso/" target=&ldquo;blank&rdquo;><b>Anthony Man-Cho So</b></a> at <a href="https://www.cuhk.edu.hk/" target=&ldquo;blank&rdquo;><b>The Chinese University of Hong Kong</b></a>.
</p>
<h2>Research Interests </h2>
<p>Broadly speaking, my research interest lies in the intersects of <b>optimization</b>, <b>machine learning</b>, and <b>data science</b>. Currently, I am devoted to understanding <b>mathematical foundations of deep learning models</b>, including supervised learning models, diffusion models, and large language models. I mainly study how low-complexity structures (e.g., low-rankness, sparsity, over-parameterization) in practical problems lead to favorable optimization properties and use them to mitigate the challenges caused by worst-case scenarios, enable efficient optimization, and improve our understanding of learning phenomena. 


</p>
<p><b> Feel free to email me if you are interested in my research. Remote collaboration is also welcome!</b> 
</p>
<h2>What's New</h2>
<ul>
<li><p>[June 2025] <a href="https://arxiv.org/abs/2506.20344" target=&ldquo;blank&rdquo;><b>One paper</b></a> on the global loss landscape analyais of deep matrix factorization is posted! 
</p>
</li>
<li><p>[May 2025] <a href="https://arxiv.org/pdf/2506.03790" target=&ldquo;blank&rdquo;><b>Our paper</b></a> on understanding the mechenism of transformers has been accepted by <b>ICML 2025</b>! 
</p>
</li>
<li><p>[Apr 2025] Our recent works on studying the generalization of diffusion models appears on <a href="https://www.siam.org/publications/siam-news/articles/generalization-of-diffusion-models-principles-theory-and-implications/" target=&ldquo;blank&rdquo;><b>SIAM News Blog</b></a>!
</p>
</li>
<li><p>[Mar 2025] <a href="https://arxiv.org/pdf/2503.19859" target=&ldquo;blank&rdquo;><b>A tutorial paper</b></a> on understanding the role of low-rank structures in the training and adaptation of deep learning models is posted! 
</p>
</li>
<li><p>[Mar 2025] I will attend the <a href="https://cpal.cc/" target=&ldquo;blank&rdquo;><b>Conference on Parsimony and Learning</b></a> at Stanford University from March 24-27! 
</p>
</li>
<li><p>[Feb 2025] <a href="https://arxiv.org/abs/2502.11152" target=&ldquo;blank&rdquo;><b>One paper</b></a> on the local error bound of deep linear networks is posted! 
</p>
</li>
<li><p>[Jan 2025] <a href="https://arxiv.org/pdf/2502.05743" target=&ldquo;blank&rdquo;><b>One paper</b></a> on the representation capabilities of diffusion models is posted! 
</p>
</li>
<li><p>[Jan 2025] <a href="https://arxiv.org/pdf/2301.00423v4" target=&ldquo;blank&rdquo;><b>One paper</b></a> is accepted by <b>INFORMS Journal on Computing</b>! 
</p>
</li>
<li><p>[Jan 2025] <a href="https://arxiv.org/pdf/2501.02364" target=&ldquo;blank&rdquo;><b>One paper</b></a> on the separation capabilities of shallow nonlinear networks is posted! 
</p>
</li>
<li><p>[Jan 2025] I will give a presentaion in <a href="https://sites.google.com/view/minds-seminar/home" target=&ldquo;blank&rdquo;><b>1W-MINDS Seminar</b></a> online at 5PM (Beijing Time) on Jan 9, 2025! 
</p>
</li>
<li><p>[Jan 2025] I will give a presenation in <a href="https://ims.nus.edu.sg/events/ims_forummath2025_am/" target=&ldquo;blank&rdquo;><b>IMS Young Mathematical Scientists Forum</b></a> at National University of Singapore! 
</p>
</li>
<li><p>[Dec 2024] <a href="https://arxiv.org/pdf/2412.07909" target=&ldquo;blank&rdquo;><b>One paper</b></a> on understanding modality gap in multimodal learning is posted! 
</p>
</li>
<li><p>[Dec 2024] I will serve as an <b>area chair</b> of <a href="https://cpal.cc/" target=&ldquo;blank&rdquo;><b>CAPL 2025</b></a>!
</p>
</li>
<li><p>[Oct 2024] I will chair a session and give a lecture presentation on diffusion models in <a href="https://cmsworkshops.com/Asilomar2024/view_paper.php?PaperNum=1469" target=&ldquo;blank&rdquo;><b>Asilomar 2024</b></a> at Pacific Grove, CA!
</p>
</li>
<li><p>[Oct 2024] <b>Two paper</b> on diffusion models get accepted by <a href="https://sites.google.com/view/m3l-2024/home?authuser=0" target=&ldquo;blank&rdquo;><b>NeurIPS Workshop on Mathematics of Modern Machine Learning</b></a>! 
</p>
</li>
<li><p>[Sep 2024] I will give a talk on diffusion models in <a href="https://ece.engin.umich.edu/event/tbd-24" target=&ldquo;blank&rdquo;><b>ECE Communications and Signal Processing Seminars</b></a> at the University of Michigan! 


</p>
</li>
<li><p>[May 2024] <b>Five papers</b> [<a href="https://arxiv.org/pdf/2406.01909" target=&ldquo;blank&rdquo;><b>paper1</b></a>, <a href="http://arxiv.org/abs/2406.04112" target=&ldquo;blank&rdquo;><b>paper2</b></a>, <a href="https://arxiv.org/pdf/2310.05264.pdf" target=&ldquo;blank&rdquo;><b>paper3</b></a>, <a href="https://arxiv.org/pdf/2310.05351.pdf" target=&ldquo;blank&rdquo;><b>paper4</b></a>, <a href="https://arxiv.org/pdf/2406.05822" target=&ldquo;blank&rdquo;><b>paper5</b></a>]  are accepted by <b>ICML 2024</b>! 










</p>
</li>
</ul>
<h2>Preprints (&lsquo;&lsquo;*&rsquo;&rsquo; denotes equal contribution, &lsquo;&lsquo;\(\dagger\)&rsquo;&rsquo; denotes corresponding author.)</h2>
<ul>
<li><p>(<font color=red size=+0.5><b>\(\alpha\)-\(\beta\) order</b></font>) Laura Balzano\(^\dagger\), Tianjiao Ding\(^\dagger\), Benjamin D. Haeffele, Soo Min Kwon, Qing Qu, <b>Peng Wang</b>\(^\dagger\), Zhangyang Wang, Can Yaras. An Overview of Low-Rank Structures in the Training and Adaptation of Large Models. Under review in <b><i>IEEE Signal Processing Magazine</i></b>, 2025. [<a href="https://arxiv.org/pdf/2503.19859" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>(<font color=red size=+0.5><b>\(\alpha\)-\(\beta\) order</b></font>) Po Chen, Rujun Jiang, <b>Peng Wang</b>\(^\dagger\). Error Bound Analysis for the Regularized Loss of Deep Linear Neural Networks, 2025. [<a href="https://arxiv.org/pdf/2502.11152" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>*, Xiao Li*, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, Qing Qu. Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination. <b>Minor revision</b> in <b><i>Journal of Machine Learning Research</i></b>, 2024. [<a href="https://arxiv.org/pdf/2311.02960.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Xiao Li*, Zekai Zhang*, Xiang Li, Siyi Chen, Zhihui Zhu, <b>Peng Wang</b>\(^\dagger\), Qing Qu. Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling, 2025. Under review in <b>NeurIPS 2025</b>. [<a href="https://arxiv.org/pdf/2502.05743" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p>Alec S Xu, Can Yaras, <b>Peng Wang</b>, Qing Qu. Understanding How Nonlinear Layers Create Linearly Separable Features for Low-Dimensional Data, 2025. Under review in <b>SIAM Journal on Mathematics of Data Science</b>. [<a href="https://arxiv.org/pdf/2501.02364" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>*, Huijie Zhang*, Zekai Zhang, Siyi Chen, Yi Ma, Qing Qu. Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering, 2024. [<a href="https://arxiv.org/pdf/2409.02426" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
<ul>
<li><p>Accepted by <b>NeurIPS 2024 M3L Workshop</b> &amp; <b>ICLR 2025 DeLTa Workshop</b>.
</p>
</li></ul>
</li>
<li><p>Can Yaras*, <b>Peng Wang</b>*, Wei Hu, Zhihui Zhu, Laura Balzano, Qing Qu. The Law of Parsimony in Gradient Descent for Learning Deep Linear Networks, 2023. To be submitted. [<a href="https://arxiv.org/pdf/2306.01154.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Taoli Zheng, <b>Peng Wang</b>, Anthony Man-Cho So. A Linearly Convergent Algorithm for Rotationally Invariant L1-Norm Principal Component Analysis, 2022. [<a href="https://arxiv.org/pdf/2210.05066.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
</ul>
<h2>Journal Papers </h2>
<ul>
<li><p><b>Peng Wang</b>, Rujun Jiang, Qingyuan Kong, Laura Balzano. A Proximal Difference-of-Convex Algorithm for Sample Average Approximation of Chance Constrained Programming. Accepted for publication in <b><i>INFORMS Journal on Computing</i></b>, 2025. [<a href="https://pubsonline.informs.org/doi/abs/10.1287/ijoc.2024.0648" target=&ldquo;blank&rdquo;><b>paper</b></a>, <a href="https://github.com/peng8wang/2024.0648" target=&ldquo;blank&rdquo;><b>code</b></a>] 
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Anthony Man-Cho So. Linear Convergence of Proximal Alternating Minimization Method with Extrapolation for L1-Norm Principal Component Analysis. <b><i>SIAM Journal on Optimization</i></b> (2023) 33(2):684-712. [<a href="https://arxiv.org/pdf/2107.07107.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p><b>Peng Wang</b>, Zirui Zhou, Anthony Man-Cho So. Non-Convex Exact Community Recovery in Stochastic Block Model. <b><i>Mathematical Programming, Series A</i></b> (2022) 195(1-2):793-829. [<a href="https://arxiv.org/pdf/2006.15843v4.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
</ul>
<h2>Conference Papers</h2>
<ul>
<li><p><b>Peng Wang</b>, Yifu Lu, Yaodong Yu, Druv Pai, Qing Qu, Yi Ma. Attention-Only Transformers via Unrolled Subspace Denoising. Accepted by <b><i>ICML 2025</i></b>. [<a href="https://arxiv.org/pdf/2506.03790" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Can Yaras*, Siyi Chen*, Peng Wang, Qing Qu. Explaining and Mitigating the Modality Gap in Contrastive Multimodal Learning. <b><i>CAPL 2025</i></b>. [<a href="https://arxiv.org/pdf/2412.07909" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p>Siyi Chen*, Huijie Zhang*, Minzhe Guo, Yifu Lu, <b>Peng Wang</b>, Qing Qu. Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing. <b><i>NeurIPS 2024</i></b>. [<a href="https://arxiv.org/pdf/2409.02374" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Druv Pai, Yaodong Yu, Zhihui Zhu, Qing Qu, Yi Ma. A Global Geometric Analysis of Maximal Coding Rate Reduction. <b><i>ICML 2024</i></b>. [<a href="https://arxiv.org/pdf/2406.01909" target=&ldquo;blank&rdquo;><b>paper</b></a>]  
</p>
</li>
<li><p>Can Yaras, <b>Peng Wang</b>, Laura Balzano, Qing Qu. Compressible Dynamics in Deep Overparameterized Low-Rank Learning &amp; Adaptation. <b><i>ICML 2024</i></b> (<font color=red size=+0.5><b>Oral, acceptance rate: 1.52%</b></font>). [<a href="http://arxiv.org/abs/2406.04112" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Huikang Liu*, <b>Peng Wang</b>*, Longxiu Huang, Qing Qu, Laura Balzano. Matrix Completion with ReLU Sampling. <b><i>ICML 2024</i></b>. [<a href="https://arxiv.org/pdf/2406.05822" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Jiachen Jiang, Jinxin Zhou, <b>Peng Wang</b>, Qing Qu, Dustin Mixon, Chong You, Zhihui Zhu. Generalized Neural Collapse for a Large Number of Classes. <b><i>ICML 2024</i></b>. [<a href="https://arxiv.org/pdf/2310.05351.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, <b>Peng Wang</b>, Liyue Shen, and Qing Qu. The Emergence of Reproducibility and Consistency in
Diffusion Models. <b><i>ICML 2024</i></b>. [<a href="https://arxiv.org/pdf/2310.05264.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p>Can Yaras*, <b>Peng Wang</b>*, Wei Hu, Zhihui Zhu, Laura Balzano, Qing Qu. Invariant Low-Dimensional Subspaces in Gradient Descent for Learning Deep Matrix Factorizations. <b><i>NeurIPS M3L Workshop 2023</i></b>. [<a href="https://openreview.net/pdf?id=4pPnQqUMLS" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Jinxin Wang, Yuen-Man Pun, Xiaolu Wang, <b>Peng Wang</b>, Anthony Man-Cho So. Projected Tensor Power Method for Hypergraph Community Recovery. <b><i>ICML 2023</i></b>.  [<a href="https://openreview.net/pdf?id=CcDKqUR546" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>*, Huikang Liu*, Can Yaras*, Laura Balzano, Qing Qu. Linear Convergence Analysis of Neural Collapse with Unconstrained Features. NeurIPS Workshop on Optimization for Machine Learning, <b><i>NeurIPS OPT Workshop 2022</i></b>. [<a href="https://openreview.net/pdf?id=WC9im-M_y5" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Can Yaras*, <b>Peng Wang</b>*, Zhihui Zhu, Laura Balzano, Qing Qu. Neural Collapse with Normalized Features: A Geometric Analysis over the Riemannian Manifold. <b><i>NeurIPS 2022</i></b>. [<a href="https://arxiv.org/pdf/2209.09211.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Anthony Man-Cho So, Laura Balzano. Convergence and Recovery Guarantees of the K-Subspaces Method for Subspace Clustering. <b><i>ICML 2022</i></b>. [<a href="https://arxiv.org/pdf/2206.05553.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p>Xiaolu Wang, <b>Peng Wang</b>, Anthony Man-Cho So. Exact Community Recovery over Signed Graphs. <b><i>AISTATS 2022</i></b>. [<a href="https://arxiv.org/pdf/2202.12255.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Zirui Zhou, Anthony Man-Cho So. Optimal Non-Convex Exact Recovery in Stochastic Block Model via Projected Power Method. <b><i>ICML 2021</i></b>. [<a href="https://arxiv.org/pdf/2106.05644.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>*, Zirui Zhou*, Anthony Man-Cho So. A Nearly-Linear Time Algorithm for Exact Community Recovery in Stochastic Block Model. <b><i>ICML 2020</i></b>. [<a href="http://proceedings.mlr.press/v119/wang20ac/wang20ac.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Anthony Man-Cho So. Globally Convergent Accelerated Proximal Alternating Maximization Method for L1-Principal Component Analysis. <b><i>ICASSP 2019</i></b> (IEEE SPS Student Travel Award). [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8682499" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Huikang Liu, <b>Peng Wang</b>, Anthony Man-Cho So. Fast First-Order Methods for the Massive Robust Multicast Beamforming Problem with Interference Temperature Constraints. <b><i>ICASSP 2019</i></b>. [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683524" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2025-06-26 09:15:54 China Standard Time, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
